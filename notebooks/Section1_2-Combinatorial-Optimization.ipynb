{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinatorial Optimization\n",
    "\n",
    "Some optimization problems are ***combinatorial***, in the sense that there are $p$ items that can be ordered or combined in many different ways, some ways being better than others according to a set of specified criteria.\n",
    "\n",
    "As an example, consider the deceptively simple traveling salesman problem (TSP), which attempts to optimize the hypothetical path of a salesman who is required to visit each of a set of cities once, then return home. Assuming travel is the same distance irrespective of travel direction, there are:\n",
    "\n",
    "$$\\frac{(p-1)!}{2}$$\n",
    "\n",
    "possible routes. So, 5 cities have 120 possible routes, 10 cities have 181,440 routes, 50 cities have $3 \\times 10^{64}$ routes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-26933ebe155c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "#Basemap not compatible with Python 3.5\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "def parse_latlon(x):\n",
    "    d, m, s = map(float, x.split(':'))\n",
    "    ms = m/60. + s/3600.\n",
    "    if d<0:\n",
    "        return d - ms\n",
    "    return d + ms\n",
    "\n",
    "cities =  pd.read_csv('../data/brasil_capitals.txt', \n",
    "                      names=['city','lat','lon'])[['lat','lon']].applymap(parse_latlon)\n",
    "\n",
    "xmin, xmax, ymin, ymax = -70.0, -20.0, -40.0, 10.0\n",
    "\n",
    "fig = plt.figure(figsize=(11.7,8.3))\n",
    "bm = Basemap(projection='merc', \\\n",
    "             llcrnrlon=xmin, llcrnrlat=ymin, \\\n",
    "             urcrnrlon=xmax, urcrnrlat=ymax, \\\n",
    "             lon_0=0.5*(xmax + xmin), lat_0=0.5*(ymax + ymin), \\\n",
    "             resolution='l', area_thresh=1000000)\n",
    "    \n",
    "bm.drawcoastlines(linewidth=1.5)\n",
    "bm.bluemarble()\n",
    "bm.drawcountries(linewidth=2)\n",
    "bm.drawstates(linewidth=1)\n",
    "\n",
    "for i,c in cities.iterrows():\n",
    "    bm.plot(-c['lat'], c['lon'], 'ro', latlon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty of a problem of size $p$ can be quantified by ***the number of calculations needed to solve the worst-case scenario using the best available algorithm***.\n",
    "\n",
    "We can bound the number of operations using the \"Big-O\" notation, which is a relative representation of the complexity of an algorithm. It can be used to compare similar tasks doing similar sorts of operations on similar data. \n",
    "\n",
    "So, in general we use the notation:\n",
    "\n",
    "$$\\mathcal{O}(h(p))$$\n",
    "\n",
    "For example, if $h$ is polynomial in $p$, then the algorithm is polynomial.\n",
    "\n",
    "Given what we showed above, the TSP is $\\mathcal{O}(p!)$ or factorial complexity. If we were able to solve a 20-city problem in 1 minute using a particular algorithm, then we could solve:\n",
    "\n",
    "* $p=21$ in 21 minutes\n",
    "* $p=25$ in 12.1 years\n",
    "* $p=30$ in 207 million years\n",
    "* $p=50$ in $2.4 \\times 10^{40}$ years\n",
    "\n",
    "contrast this with a polynomial problem such as $\\mathcal{O}(p^2)$:\n",
    "\n",
    "* $p=21$ in 70 seconds\n",
    "* $p=25$ in 1.57 minutes\n",
    "* $p=30$ in 2.25 minutes\n",
    "* $p=50$ in 6.25 minutes\n",
    "\n",
    "Clearly, finding an algorithm that reduces solving a particular problem from factorial to polynomial time is advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xkcd travelling salesman](http://imgs.xkcd.com/comics/travelling_salesman_problem.png)\n",
    "\n",
    "(via [xkcd](http://xkcd.com/399/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics\n",
    "\n",
    "Unfortunately, there is no known combinatorial optimization algorithm for obtaining an optimal solution to the TSP in polynomial time. Instead, we must turn to ***heuristics***, which have no guarantee of a global maximum, but in practice tend to yield *good* results in a reasonable time. Thus, we are trading off global optimality for a little speed.\n",
    "\n",
    "Heuristics have two notable characteristics:\n",
    "\n",
    "* **iteration**: candidate solutions are incrementally improved\n",
    "* **localization**: search for improved solutions are restricted to a local neighborhood of the current solution\n",
    "\n",
    "This ***local search*** approach encompasses several specific techniques, some of which we will explore here. For a given candidate solution vector $\\mathbf{\\theta}^{(t)}$ at iteration $t$, we might change components $\\theta_i$ to propose an updated solution $\\mathbf{\\theta}^{(t+1)}$. Limiting the number of changes keeps $\\mathbf{\\theta}^{(t+1)}$ in the *neighborhood* of $\\mathbf{\\theta}^{(t)}$. We refer to $k$ changes to the candidate solution as a **k-change** and the set of possible new candidates as the *k-neighborhood*.\n",
    "\n",
    "A sensible approach for updating a candidate solution is to choose the best candidate from the neighborhood; this is called ***steepest ascent***. The selection of any improved candidate is called an *ascent*. However, choosing the steepest ascent from a neighborhood may not be globally optimal if, for example, it takes us toward a local maximum at the cost of missing a global maximum. An algorithm that uses a steepest ascent strategy in the context of local search is called a *greedy* algorithm.\n",
    "\n",
    "In order to attain a global maximum (or globally-competitive solution), it makes sense to  occasionaly choose an candidate solution that is not the best-in-neighborhood. In other words, to move from one peak to another (higher) peak, one must pass through valleys.\n",
    "\n",
    "Since local search algorithms can easily converge to local optima, one *ad hoc* approach to improve exploration of the parameter space is to use ***random starts***. This simply involves running the local search multiple times, each time initializing the starting point randomly over $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Baseball salaries\n",
    "\n",
    "Perhaps we wish to develop a predictive model of baseball salaries, using various player statistics as predictors. To keep things simple, we will use linear regression on everyday players (non-pitchers) only. We seek an optimal (parsimonious) subset of a set that includes 27 candidate statistics. \n",
    "\n",
    "A brute-force search of the entire parameter space would involve comparing $2^{27} = 134,217,728$ models, so we will instead use random-starts local search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "baseball = pd.read_table(\"../data/textbook/baseball.dat\", sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>average</th>\n",
       "      <th>obp</th>\n",
       "      <th>runs</th>\n",
       "      <th>hits</th>\n",
       "      <th>doubles</th>\n",
       "      <th>triples</th>\n",
       "      <th>homeruns</th>\n",
       "      <th>rbis</th>\n",
       "      <th>walks</th>\n",
       "      <th>...</th>\n",
       "      <th>rbisperso</th>\n",
       "      <th>walksperso</th>\n",
       "      <th>obppererror</th>\n",
       "      <th>runspererror</th>\n",
       "      <th>hitspererror</th>\n",
       "      <th>hrspererror</th>\n",
       "      <th>soserrors</th>\n",
       "      <th>sbsobp</th>\n",
       "      <th>sbsruns</th>\n",
       "      <th>sbshits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3300</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.302</td>\n",
       "      <td>69</td>\n",
       "      <td>153</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>104</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>17.2500</td>\n",
       "      <td>38.2500</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>320</td>\n",
       "      <td>1.208</td>\n",
       "      <td>276</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2600</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.335</td>\n",
       "      <td>58</td>\n",
       "      <td>111</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.0838</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>27.7500</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.337</td>\n",
       "      <td>54</td>\n",
       "      <td>115</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>73</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6293</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>19.1667</td>\n",
       "      <td>2.8333</td>\n",
       "      <td>696</td>\n",
       "      <td>2.022</td>\n",
       "      <td>324</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2475</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.292</td>\n",
       "      <td>59</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.3594</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>2.6818</td>\n",
       "      <td>5.8182</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>1408</td>\n",
       "      <td>6.132</td>\n",
       "      <td>1239</td>\n",
       "      <td>2688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2313</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.346</td>\n",
       "      <td>87</td>\n",
       "      <td>169</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0943</td>\n",
       "      <td>1.3208</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>9.6667</td>\n",
       "      <td>18.7778</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>477</td>\n",
       "      <td>1.038</td>\n",
       "      <td>261</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary  average    obp  runs  hits  doubles  triples  homeruns  rbis  \\\n",
       "0    3300    0.272  0.302    69   153       21        4        31   104   \n",
       "1    2600    0.269  0.335    58   111       17        2        18    66   \n",
       "2    2500    0.249  0.337    54   115       15        1        17    73   \n",
       "3    2475    0.260  0.292    59   128       22        7        12    50   \n",
       "4    2313    0.273  0.346    87   169       28        5         8    58   \n",
       "\n",
       "   walks   ...     rbisperso  walksperso  obppererror  runspererror  \\\n",
       "0     22   ...        1.3000      0.2750       0.0755       17.2500   \n",
       "1     39   ...        0.9565      0.5652       0.0838       14.5000   \n",
       "2     63   ...        0.6293      0.5431       0.0562        9.0000   \n",
       "3     23   ...        0.7812      0.3594       0.0133        2.6818   \n",
       "4     70   ...        1.0943      1.3208       0.0384        9.6667   \n",
       "\n",
       "   hitspererror  hrspererror  soserrors  sbsobp  sbsruns  sbshits  \n",
       "0       38.2500       7.7500        320   1.208      276      612  \n",
       "1       27.7500       4.5000        276   0.000        0        0  \n",
       "2       19.1667       2.8333        696   2.022      324      690  \n",
       "3        5.8182       0.5455       1408   6.132     1239     2688  \n",
       "4       18.7778       0.8889        477   1.038      261      507  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xbbc5080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFVCAYAAADG2GfeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1d9zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb1ac748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseball.salary.hist(grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since players salaries are strongly skewed, we will log-transform the response variable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8.101678\n",
       "1    7.863267\n",
       "2    7.824046\n",
       "3    7.813996\n",
       "4    7.746301\n",
       "Name: salary, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = baseball.copy()\n",
    "logsalary = predictors.pop('salary').apply(np.log) #return the salary column and remove it from predictors dataframe\n",
    "logsalary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the dimensions of the data frame\n",
    "nrows, ncols = predictors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this example, we will start the algorithm from 5 different starting positions in the model space $\\Theta$, and will run each algorithm for 15 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nstarts = 5\n",
    "iterations = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate random starting candidate models by generating random indicators for each of the 27 predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False,  True, False, False, False,  True, False,\n",
       "        False, False,  True, False,  True, False,  True,  True,  True,\n",
       "        False, False, False, False, False, False,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        False,  True,  True,  True, False, False, False,  True,  True,\n",
       "        False, False,  True, False,  True,  True,  True, False,  True],\n",
       "       [False, False, False,  True,  True, False,  True,  True, False,\n",
       "         True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True],\n",
       "       [False, False,  True, False,  True,  True, False,  True,  True,\n",
       "        False, False, False, False, False,  True, False, False,  True,\n",
       "        False,  True, False, False,  True,  True, False,  True, False],\n",
       "       [ True, False,  True, False,  True,  True,  True,  True, False,\n",
       "         True, False, False, False,  True, False,  True,  True,  True,\n",
       "        False,  True,  True,  True, False, False,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly initialize starting values of runs\n",
    "runs = np.random.binomial(1, 0.5, ncols*nstarts).reshape((nstarts,ncols)).astype(bool)\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of model parsimony, we will use Akaike's Information Criterion (AIC), which is an information-theoretic model selection criterion. It balances model fit (likelihood) with model size by penalizing the addition of parameters. \n",
    "\n",
    "\\\\[AIC = n \\log(SSE/n) + 2k\\\\]\n",
    "\n",
    "Thus models that add parameters that do not appreciably inrpove model fit will be down-weighted by AIC. Better models have a lower AIC value for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aic = lambda g, X, y: len(y) * np.log(sum((g.predict(X) - y)**2)/len(y)) + 2*g.rank_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration of the algorithm, we examine models in the 1-neighborhood of the current model by adding or dropping each predictor in turn, fitting the model and calculating the associated AIC value. We then select the model with the lowest AIC in the neighborhood and set that to the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Create a new array without initializing entries\n",
    "runs_aic = np.empty((nstarts, iterations))\n",
    "\n",
    "for i in range(nstarts): #nstarts=5\n",
    "    \n",
    "    run_current = runs[i]  # ith row of runs(T/F matrix)\n",
    "    \n",
    "    for j in range(iterations):  #iterations=15\n",
    "        \n",
    "        # Extract current set of predictors\n",
    "        run_vars = predictors[predictors.columns[run_current]] #pick the columns to run the model\n",
    "        g = LinearRegression().fit(X=run_vars, y=logsalary)\n",
    "        run_aic = aic(g, run_vars, logsalary)\n",
    "        run_next = run_current\n",
    "        \n",
    "        # Test all models in 1-neighborhood and select lowest AIC (i.e. change 1 variable from previous model)\n",
    "        for k in range(ncols):\n",
    "            run_step = run_current.copy()\n",
    "            run_step[k] = not run_current[k] #inverse of the kth element in ith row of runs(T/F matrix)\n",
    "            run_vars = predictors[predictors.columns[run_step]] #pick columns to run the alternative model \n",
    "            g = LinearRegression().fit(X=run_vars, y=logsalary)\n",
    "            step_aic = aic(g, run_vars, logsalary)\n",
    "            if step_aic < run_aic:\n",
    "                run_next = run_step.copy()\n",
    "                run_aic = step_aic\n",
    "            \n",
    "        run_current = run_next.copy()\n",
    "        runs_aic[i,j] = run_aic\n",
    "        \n",
    "    runs[i] = run_current\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the change in AIC, we see that all the runs tend to converge to the same (or equivalent) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xb60ea90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFkCAYAAADv13iSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1ERERA7GoCYiInIwBjUREZGD/X9KHjCIx/IbTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb61ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(-1*runs_aic).T.plot(grid=False)\n",
    "plt.xlabel('iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows the number of times a given predictor was used across all 5 solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arbitration     5\n",
       "runs            5\n",
       "freeagent       5\n",
       "runsperso       5\n",
       "sos             5\n",
       "sbsobp          5\n",
       "sbsruns         4\n",
       "rbis            4\n",
       "triples         3\n",
       "hitsperso       3\n",
       "hitspererror    2\n",
       "obp             2\n",
       "obppererror     2\n",
       "hits            2\n",
       "soserrors       2\n",
       "rbisperso       2\n",
       "walks           1\n",
       "homeruns        1\n",
       "sbshits         1\n",
       "errors          1\n",
       "hrsperso        1\n",
       "runspererror    1\n",
       "average         1\n",
       "sbs             0\n",
       "doubles         0\n",
       "walksperso      0\n",
       "hrspererror     0\n",
       "dtype: int32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(runs.sum(0), index=predictors.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing\n",
    "\n",
    "Annealing is a metallurgic process that involves heating a material beyond its critical temperature, maintaining a suitable temperature, and then cooling. If the process is followed according to a schedule appropriate to the material, annealing increases its ductility and reduces the number of defects. ***Simulated annealing*** uses an analogous process to pursue an optimial solution of a function over a large search space, in the sense that at high  \"temperatures\" (large probabilities) solutions are proposed from more distant regions of the  parameter space relative to the current solution. Similarly, \"cooling\" slowly reduces the number of exploratory proposals in order to converge on a (hopefully) global optimum. Thus, there is an initial period of liberal exploration, which gradually decreases to resemble hill-climbing procedures that we have already seen.\n",
    "\n",
    "A generic simulated annealing algorithm proceeds as follows:\n",
    "\n",
    "1. Initialize $t=0$, $\\theta^{(t=0)}$, temperature $\\tau^{(0)}$\n",
    "2. Iterate until convergence:\n",
    "\n",
    "    a. Select candidate solution $\\theta^{\\prime}$ from neighborhood of $\\theta^{(t)}$ \n",
    "    according to proposal g($\\theta^{(t)})$  \n",
    "    b. Set $\\theta^{(t+1)} = \\theta^{\\prime}$ with probability:\n",
    "    \n",
    "    $$\\alpha = \\min\\left(1, \\exp\\left[\\frac{f(\\theta^{(t)})-f(\\theta^{\\prime})}{\\tau^{(j)}}\\right]\\right)$$\n",
    "    \n",
    "    otherwise, set $\\theta^{(t+1)} = \\theta^{(t)}$  \n",
    "    c. Repeat (a) and (b) for $m_j$ iterations  \n",
    "    d. Increment $j$, update $\\tau^{(j)}$ and $m_j$ according to cooling schedule\n",
    "    \n",
    "This algorithm can be halted once the minimum temperature is reached. The temerature $\\tau^{(j)}$ should slowly decrease, while the time spent at each temperature $m_j$ should correspondingly increase.\n",
    "\n",
    "Notice that this algorithm implies that though superior candidates are *always* adopted when proposed, inferior solutions are also accepted, but with some probability that is related to its quality relative to the current solution. This is what allows for exploration of the parameter space, and the escape from local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposals\n",
    "\n",
    "A variety of proposal and neighborhood strategies can be effective, depending on the structure of the problem at hand. One constraint on generating proposed solutions is that all pairs of solutions $(\\theta^{(i)}, \\theta^{(j)}) \\in \\Theta, i \\ne j$ be able to ***communicate***. That is, there must exist some finite sequence of solutions that can be generated starting at $\\theta^{(i)}$ such that we eventually reach $\\theta^{(j)}$.\n",
    "\n",
    "The simplest example of a proposal strategy that allows for communication among solutions is to sample uniformly from the $k$-neighborhood of the current solution.\n",
    "\n",
    "### Cooling and Convergence\n",
    "\n",
    "Unlike some metaheuristics, the limiting behavior of simulated annealing is pretty well-known. Within each temperature regime, the SA algorithm produces a homogeneous Markov chain (since the transition probabilties do not change at each step), becoming non-homogeneous whenever a cooling event is triggered. If we generate symmetric proposals, such that the probability of proposing $\\theta^{(i)}$ from $\\theta^{(j)}$ is equal to that of proposing $\\theta^{(j)}$ from $\\theta^{(i)}$, then the *stationary* distribution of the Markov chain is:\n",
    "\n",
    "$$p_{\\tau}(\\theta) \\propto \\exp[-f(\\theta)/\\tau]$$\n",
    "\n",
    "So, in the limit, as we run the Markov chain, it converges to $p_{\\tau}(\\theta)$. During the SA algorithm, we want to run the chain long enough so that it is close to its stationary distribution before cooling. How long is \"long enough\" usually takes some experimentation using trial runs.\n",
    "\n",
    "In general, we adopt a cooling schedule that sets the temperature at period $j$ according to $\\tau^{(j)} = f_{\\alpha}(\\tau^{(j-1)})$ and the number of iterations in period $j$ according to $m_j = f_\\beta(m_{j-1})$. Some specific examples include:\n",
    "\n",
    "* $m_j = 1$ for all $j$, with very slow cooling via $\\tau^{(j)} = \\tau^{(j-1)}/( 1 + \\alpha \\tau^{(j-1)})$ for some small chosen $\\alpha$\n",
    "* $\\tau^{(j)} = \\alpha \\tau^{(j-1)}$ for some $\\alpha \\lt 1$ with $m_j = \\beta m_{j-1}$ for some $\\beta \\lt 1$\n",
    "\n",
    "One approach is to choose an initial temperature $\\tau^{(0)}$ so that $p$ is close to 1 for all combinations of $\\theta^{(i)}$ and $\\theta^{(j)}$, which provides any point a reasonable chance of being visited early in the simulation. \n",
    "\n",
    "The appropriate choice for $m_j$ is a tradeoff between performance and speed: a large number of steps can produce a more accurate solution, but requires additional computation. Evidence suggests that long runs at high temperatures is not optimal. For most problems, good cooling schedules invlove a rapid decrease in temperature early in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Baseball salaries\n",
    "\n",
    "Let's revisit the baseball salaries example, this time using simulated anealling to search for an optimal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau_start = 10\n",
    "cooling_schedule = [tau_start]*60 + [tau_start/2]*120 + [tau_start/10]*240\n",
    "aic_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the annealing run and temperature schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution_current = solution_best = np.random.binomial(1, 0.5, ncols).astype(bool)\n",
    "solution_vars = predictors[predictors.columns[solution_current]]\n",
    "g = LinearRegression().fit(X=solution_vars, y=logsalary)\n",
    "aic_best = aic(g, solution_vars, logsalary)\n",
    "aic_values.append(aic_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simulated annealing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tau in cooling_schedule:\n",
    "        \n",
    "    # Random change 1-neighborhood\n",
    "    flip = np.random.randint(0, ncols)\n",
    "    solution_current[flip] = not solution_current[flip]\n",
    "    solution_vars = predictors[predictors.columns[solution_current]]\n",
    "    g = LinearRegression().fit(X=solution_vars, y=logsalary)\n",
    "    aic_step = aic(g, solution_vars, logsalary)\n",
    "    alpha = min(1, np.exp((aic_values[-1] - aic_step)/tau))\n",
    "\n",
    "    if ((aic_step < aic_values[-1]) or (np.random.uniform() < alpha)):\n",
    "        # Accept proposed solution\n",
    "        aic_values.append(aic_step)\n",
    "        if aic_step < aic_best:\n",
    "            # Replace previous best with this one\n",
    "            aic_best = aic_step\n",
    "            solution_best = solution_current.copy()\n",
    "    else:\n",
    "        # Revert solution\n",
    "        solution_current[flip] = not solution_current[flip]\n",
    "        aic_values.append(aic_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(aic_values)\n",
    "plt.xlim(0, len(aic_values))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('AIC')\n",
    "print('Best AIC: {0}\\nBest solution: {1}\\nDiscovered at iteration {2}'.format(aic_best, \n",
    "            np.where(solution_best==True),\n",
    "            np.where(aic_values==aic_best)[0][0]))\n",
    "plt.plot(np.where(aic_values==aic_best)[0][0], aic_best, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this solution to one of the local search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(runs[-1]==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a function for simulated annealing that uses the above code, but allows for custom cooling schedules (and other parameters) to be passed. Try running the function with the continuous cooling schedule (e.g. cooling by 10% each iteration) described above and comapare the results to the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithms\n",
    "\n",
    "From a metallurgic metaphor, we turn to an ecological one: The genetic algorithm (GA) metaheuristic mimics the process of natural selection. In doing so, we construct a **population** of solutions that are analogous to organisms in a natural system. The relative quality of any particular solution represents its fitness, and improves its ability to pass on its desirable attributes to future generations of solutions, which are generated by combining with other viable solutions.\n",
    "\n",
    "Phenotypes in GA are candidate solutions, the components of which are coded into its genotype on the chromosome. For convenience, organisms in GA have a single chromosome. If we take our baseball salary problem as an example, the chromosome might simply consist a list of indicators for the presence of each available covariate in a particular model:\n",
    "\n",
    "    [0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, \n",
    "    1, 1, 1, 0, 1, 1, 0, 0]\n",
    "    \n",
    "this particular chromosome encodes for a model with `runs, triples, rbis, sos, freeagent, arbitration, obppererror, runspererror, hitspererror, soserrors, sbsobp` included as covariates. Thus, each locus on the chromosome is a linear model coefficient, and the two corresponding alleles are the presence and absence of that coefficient.\n",
    "\n",
    "*How would you encode the travelling salesman problem?*\n",
    "\n",
    "The key aspect of the natural selection metaphor is the concept of ***fitness***. In nature, individuals with high fitness are those who are able to pass their genes onto the next generation more successfully than other individuals in the population; typically, this relates both to the ability to survive to reproductive age, and then to reproduce successfuly. In a genetic algorithm, \"fit\" individuals are solutions of high value, as determined by our objective function of choice; for the baseball salary example we have been using, this is the AIC value of the linear model corresponding to the candidate solution. We facilitate this in the algorithm by giving high-value solutions a higher probability of propagating into the next generation than those of low probability.\n",
    "\n",
    "Let's consider another genotype,\n",
    "\n",
    "    [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, \n",
    "    0, 1, 1, 1, 1, 0, 1, 0]\n",
    "    \n",
    "Comparing this with the previous genotype, we say that the ***schema*** for this set of two genotypes is:\n",
    "\n",
    "    [0, 0, 1, 0, *, 1, 0, *, 0, 1, 0, 0, 1, 1, *, 0, *, *, 0, \n",
    "    *, 1, 1, *, 1, *, *, 0]\n",
    "    \n",
    "where wildcards (`*`) represent loci that differ among the set. If this schema (or aspects of it) confer higher fitness, then it will tend to be represented in future populations as the algorithm progresses.\n",
    "\n",
    "The key difference between GA and previous metaheuristics is that here we track more than one candidate solution simultaneously, in the form of our \"population\" of organisms, each of which encodes a solution. This population changes over time as organisms reproduce, via ***crossover*** and ***mutation***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduction and Genetic Change\n",
    "\n",
    "In a genetic algorithm, individual solutions \"reproduce\" by combining with other solutions in the population according to some rule that is fitness-based. For example, we may select pairs of parent solutions with probabilities proportional to their fitness, and combine them to generate child solutions, which replace their parents in the next generation.\n",
    "\n",
    "There are a handful of ***genetic operators*** that are used to create children from parents. The first is crossover, which randomly allocates a position on two parent chromosomes after which all the genes on the two chromosomes swap from one to the other. For example, let's arbitrarily specify a crossover after position 7 of a pair of 10-gene chromosomes (here indicated by an apostrophe):\n",
    "\n",
    "\n",
    "    0100101`001\n",
    "    \n",
    "    0001001`111\n",
    "    \n",
    "This would result in two new genotypes in the offspring of these parents:\n",
    "\n",
    "\n",
    "    0100101111\n",
    "    \n",
    "    0001001001\n",
    "    \n",
    "Notice that the schema from the two chromosomes (`0*0***01**1`) is preserved in the next generation.\n",
    "\n",
    "One way to escape the constraint of schema preservation is to apply a second genetic operator, mutation. Mutation is usually applied after crossover, and simply involves randomly changing an allele at a randomly-chosen locus, according to some specified mutation rate. Note that if mutation is too rare it reduces the rate at which the solution space is explored, and if too frequent it disrupts the inheritance of high-value schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_size = 20\n",
    "iterations = 100\n",
    "mutation_rate = .02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aic_best = []\n",
    "best_solution = []\n",
    "aic_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the population, and their corresponding fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize genotype\n",
    "current_gen = np.random.binomial(1, 0.5, pop_size*ncols).reshape((pop_size, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_fitness(aic_values):\n",
    "    P = len(aic_values)\n",
    "    aic_rank = (-aic_values).argsort().argsort()+1.\n",
    "    return 2.*aic_rank/(P*(P+1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    \n",
    "    # Get phenotype\n",
    "    current_phe = [predictors[predictors.columns[g.astype(bool)]] for g in current_gen]\n",
    "    # Calculate AIC\n",
    "    current_aic = np.array([aic(LinearRegression().fit(X=x, y=logsalary), x, logsalary) for x in current_phe])\n",
    "    # Get lowest AIC\n",
    "    aic_best.append(current_aic[np.argmin(current_aic)])\n",
    "    best_solution.append(current_gen[np.argmin(current_aic)])\n",
    "    \n",
    "    # Calculate fitness according to AIC rank\n",
    "    fitness = calculate_fitness(current_aic)\n",
    "    \n",
    "    # Choose first parents according to fitness\n",
    "    moms = np.random.choice(range(pop_size), size=int(pop_size/2), p=fitness)\n",
    "    # Choose second parents randomly\n",
    "    dads = np.random.choice(range(pop_size), size=int(pop_size/2))\n",
    "    \n",
    "    next_gen = []\n",
    "    for x,y in zip(current_gen[moms], current_gen[dads]):\n",
    "        # Crossover\n",
    "        cross = np.random.randint(0, ncols)\n",
    "        child1 = np.r_[x[:cross], y[cross:]]\n",
    "        child2 = np.r_[y[:cross], x[cross:]]\n",
    "        # Mutate\n",
    "        m1 = np.random.binomial(1, mutation_rate, size=ncols).astype(bool)\n",
    "        child1[m1] = abs(child1[m1]-1)\n",
    "        m2 = np.random.binomial(1, mutation_rate, size=ncols)\n",
    "        child2[m2] = abs(child1[m2]-1)\n",
    "        next_gen += [child1, child2]\n",
    "        \n",
    "    # Increment generation\n",
    "    current_gen = np.array(next_gen)\n",
    "    # Store AIC values\n",
    "    aic_history.append(current_aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(aic_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(aic_history):\n",
    "    plt.plot(np.ones(len(x))*i, -x, 'r.', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Wine classification\n",
    "\n",
    "Thirteen chemical measurements were carried out on each of 178 wines from three regions of Italy. These data are available in the `wine.dat` file in your `data` directory. Using one or more heuristic search methods from this lecture, partition the wines into three groups for which the total of the within-group sum of squares is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer hre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Chapter 3 of [Givens, Geof H.; Hoeting, Jennifer A. (2012-10-09). Computational Statistics (Wiley Series in Computational Statistics)](http://www.stat.colostate.edu/computationalstatistics/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
